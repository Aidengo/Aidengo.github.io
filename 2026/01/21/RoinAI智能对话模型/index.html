<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>RoinAI智能对话模型 | Aidengo‘s blog</title><meta name="author" content="曾子复"><meta name="copyright" content="曾子复"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="从零构建RoinAI对话模型：我的技术实践与心路历程在AI大模型应用落地的热潮下，我萌生了从零开发一款轻量级对话模型——RoinAI的想法。这个项目不仅是对我技术栈的一次全面检验，更是让我深入理解对话系统底层逻辑的契机。本文将完整复盘RoinAI的开发全过程、核心实现思路，以及开发中踩过的坑和解决思路，希望能给同样想上手对话模型开发的开发者一些参考。 一、项目背景与核心目标在开始编码前，我先明确了">
<meta property="og:type" content="article">
<meta property="og:title" content="RoinAI智能对话模型">
<meta property="og:url" content="https://aidengo.github.io/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Aidengo‘s blog">
<meta property="og:description" content="从零构建RoinAI对话模型：我的技术实践与心路历程在AI大模型应用落地的热潮下，我萌生了从零开发一款轻量级对话模型——RoinAI的想法。这个项目不仅是对我技术栈的一次全面检验，更是让我深入理解对话系统底层逻辑的契机。本文将完整复盘RoinAI的开发全过程、核心实现思路，以及开发中踩过的坑和解决思路，希望能给同样想上手对话模型开发的开发者一些参考。 一、项目背景与核心目标在开始编码前，我先明确了">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://aidengo.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2026-01-21T01:05:54.000Z">
<meta property="article:modified_time" content="2026-01-21T01:15:15.701Z">
<meta property="article:author" content="曾子复">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://aidengo.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RoinAI智能对话模型",
  "url": "https://aidengo.github.io/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/",
  "image": "https://aidengo.github.io/img/butterfly-icon.png",
  "datePublished": "2026-01-21T01:05:54.000Z",
  "dateModified": "2026-01-21T01:15:15.701Z",
  "author": [
    {
      "@type": "Person",
      "name": "曾子复",
      "url": "https://Aidengo.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://aidengo.github.io/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RoinAI智能对话模型',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Aidengo‘s blog</span></a><a class="nav-page-title" href="/"><span class="site-name">RoinAI智能对话模型</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">RoinAI智能对话模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-21T01:05:54.000Z" title="发表于 2026-01-21 09:05:54">2026-01-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-21T01:15:15.701Z" title="更新于 2026-01-21 09:15:15">2026-01-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="从零构建RoinAI对话模型：我的技术实践与心路历程"><a href="#从零构建RoinAI对话模型：我的技术实践与心路历程" class="headerlink" title="从零构建RoinAI对话模型：我的技术实践与心路历程"></a>从零构建RoinAI对话模型：我的技术实践与心路历程</h1><p>在AI大模型应用落地的热潮下，我萌生了从零开发一款轻量级对话模型——RoinAI的想法。这个项目不仅是对我技术栈的一次全面检验，更是让我深入理解对话系统底层逻辑的契机。本文将完整复盘RoinAI的开发全过程、核心实现思路，以及开发中踩过的坑和解决思路，希望能给同样想上手对话模型开发的开发者一些参考。</p>
<h2 id="一、项目背景与核心目标"><a href="#一、项目背景与核心目标" class="headerlink" title="一、项目背景与核心目标"></a>一、项目背景与核心目标</h2><p>在开始编码前，我先明确了RoinAI的定位：一款面向个人开发者的轻量级本地对话模型，支持基础的多轮对话、意图识别，无需依赖大型云服务，可部署在普通PC端。核心目标拆解为三点：</p>
<ol>
<li>实现基于上下文的多轮对话能力，避免单次问答的割裂感；</li>
<li>轻量化部署，模型推理耗时控制在1秒内（普通酷睿i5处理器）；</li>
<li>具备基础的意图识别能力，可区分闲聊、知识问答、指令执行三类核心意图。</li>
</ol>
<h2 id="二、项目技术栈选型"><a href="#二、项目技术栈选型" class="headerlink" title="二、项目技术栈选型"></a>二、项目技术栈选型</h2><p>技术选型是项目的基础，结合“轻量级”核心目标，我最终确定的技术栈如下：</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>技术选型</th>
<th>选型原因</th>
</tr>
</thead>
<tbody><tr>
<td>模型底座</td>
<td>清华ChatGLM-6B（量化版）</td>
<td>6B参数量级兼顾效果与部署成本，4-bit量化后可在16G内存主机运行，中文适配性优</td>
</tr>
<tr>
<td>框架</td>
<td>Hugging Face Transformers + PyTorch</td>
<td>成熟的大模型调用框架，生态完善，支持自定义修改模型推理逻辑</td>
</tr>
<tr>
<td>上下文管理</td>
<td>Python + Redis</td>
<td>Redis的哈希结构适合存储会话上下文，读写速度快，支持过期清理</td>
</tr>
<tr>
<td>意图识别</td>
<td>轻量级文本分类模型（FastText）</td>
<td>训练成本低、推理快，适配本地部署的轻量需求，无需依赖大模型做意图识别</td>
</tr>
<tr>
<td>部署封装</td>
<td>Flask + Docker</td>
<td>Flask快速搭建API接口，Docker实现环境隔离，方便一键部署</td>
</tr>
</tbody></table>
<h2 id="三、项目开发全流程"><a href="#三、项目开发全流程" class="headerlink" title="三、项目开发全流程"></a>三、项目开发全流程</h2><h3 id="阶段1：环境搭建与模型基础调用（耗时1天）"><a href="#阶段1：环境搭建与模型基础调用（耗时1天）" class="headerlink" title="阶段1：环境搭建与模型基础调用（耗时1天）"></a>阶段1：环境搭建与模型基础调用（耗时1天）</h3><p>第一步是完成基础环境的搭建，确保模型能正常跑通单次问答。</p>
<ol>
<li><strong>环境配置</strong>：创建Python虚拟环境（Python 3.9），安装依赖：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.0.1 transformers==4.29.2 accelerate==0.19.0 redis==4.5.5 flask==2.3.2 fasttext==0.9.2 docker</span><br></pre></td></tr></table></figure>
注：torch版本需匹配本地CUDA（若无GPU，安装CPU版torch）。</li>
<li><strong>模型下载与量化</strong>：从Hugging Face下载ChatGLM-6B权重，使用官方提供的量化脚本将模型量化为4-bit，减少内存占用：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).quantize(<span class="number">4</span>).half().cuda()</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 测试单次问答</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;你好&quot;</span>, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure></li>
<li><strong>验证基础功能</strong>：确保单次问答能正常返回结果，这是后续所有功能的基础。</li>
</ol>
<h3 id="阶段2：上下文管理模块开发（耗时2天）"><a href="#阶段2：上下文管理模块开发（耗时2天）" class="headerlink" title="阶段2：上下文管理模块开发（耗时2天）"></a>阶段2：上下文管理模块开发（耗时2天）</h3><p>多轮对话的核心是上下文管理，需要记录用户的历史对话，并在每次请求时携带历史信息。</p>
<ol>
<li><strong>会话设计</strong>：为每个用户生成唯一的session_id，以session_id为key，在Redis中存储对话历史（格式：[(用户问题1, 模型回答1), (用户问题2, 模型回答2)]）。</li>
<li><strong>核心逻辑实现</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化Redis连接</span></span><br><span class="line">r = redis.Redis(host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, decode_responses=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成session_id</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_session_id</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(uuid.uuid4())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储对话历史</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_history</span>(<span class="params">session_id, history</span>):</span><br><span class="line">    <span class="comment"># 将历史对话序列化为字符串存储</span></span><br><span class="line">    r.hset(<span class="string">&quot;chat_history&quot;</span>, session_id, <span class="built_in">str</span>(history))</span><br><span class="line">    <span class="comment"># 设置过期时间（24小时），避免内存占用过高</span></span><br><span class="line">    r.expire(<span class="string">&quot;chat_history&quot;</span>, <span class="number">86400</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取对话历史</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_history</span>(<span class="params">session_id</span>):</span><br><span class="line">    history_str = r.hget(<span class="string">&quot;chat_history&quot;</span>, session_id)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> history_str:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="comment"># 反序列化回列表</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">eval</span>(history_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多轮对话调用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_with_history</span>(<span class="params">session_id, question</span>):</span><br><span class="line">    history = get_history(session_id)</span><br><span class="line">    response, new_history = model.chat(tokenizer, question, history=history)</span><br><span class="line">    save_history(session_id, new_history)</span><br><span class="line">    <span class="keyword">return</span> response, new_history</span><br></pre></td></tr></table></figure></li>
<li><strong>边界处理</strong>：添加历史对话长度限制（最多保留10轮），避免历史过长导致模型推理变慢、内存溢出：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trim_history</span>(<span class="params">history, max_round=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(history) &gt; max_round:</span><br><span class="line">        <span class="keyword">return</span> history[-max_round:]</span><br><span class="line">    <span class="keyword">return</span> history</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="阶段3：意图识别模块开发（耗时3天）"><a href="#阶段3：意图识别模块开发（耗时3天）" class="headerlink" title="阶段3：意图识别模块开发（耗时3天）"></a>阶段3：意图识别模块开发（耗时3天）</h3><p>单纯的对话模型无法区分用户意图，因此需要新增意图识别模块，将用户输入分类为“闲聊”“知识问答”“指令执行”三类。</p>
<ol>
<li><strong>数据集准备</strong>：手动标注约500条样本（格式：__label__意图 文本内容），示例：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__label__闲聊 今天天气怎么样</span><br><span class="line">__label__知识问答 什么是Python装饰器</span><br><span class="line">__label__指令执行 帮我生成一个Python冒泡排序代码</span><br></pre></td></tr></table></figure></li>
<li><strong>模型训练</strong>：使用FastText训练轻量级分类模型：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = fasttext.train_supervised(<span class="built_in">input</span>=<span class="string">&quot;intent_data.txt&quot;</span>, epoch=<span class="number">20</span>, lr=<span class="number">0.1</span>, wordNgrams=<span class="number">2</span>, dim=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save_model(<span class="string">&quot;intent_model.bin&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 意图预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_intent</span>(<span class="params">text</span>):</span><br><span class="line">    intent = model.predict(text, k=<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>].replace(<span class="string">&quot;__label__&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> intent</span><br></pre></td></tr></table></figure></li>
<li><strong>意图联动</strong>：在对话流程中加入意图判断，针对不同意图调整模型回复策略（如指令执行类请求，强制模型返回结构化代码）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">roin_ai_chat</span>(<span class="params">session_id, question</span>):</span><br><span class="line">    <span class="comment"># 意图识别</span></span><br><span class="line">    intent = predict_intent(question)</span><br><span class="line">    <span class="comment"># 获取历史对话</span></span><br><span class="line">    history = get_history(session_id)</span><br><span class="line">    <span class="comment"># 不同意图的回复策略</span></span><br><span class="line">    <span class="keyword">if</span> intent == <span class="string">&quot;指令执行&quot;</span>:</span><br><span class="line">        prompt = <span class="string">f&quot;请严格按照代码格式返回，不要多余解释：<span class="subst">&#123;question&#125;</span>&quot;</span></span><br><span class="line">        response, new_history = model.chat(tokenizer, prompt, history=history)</span><br><span class="line">    <span class="keyword">elif</span> intent == <span class="string">&quot;知识问答&quot;</span>:</span><br><span class="line">        prompt = <span class="string">f&quot;请用简洁、准确的语言回答，不要闲聊：<span class="subst">&#123;question&#125;</span>&quot;</span></span><br><span class="line">        response, new_history = model.chat(tokenizer, prompt, history=history)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        response, new_history = model.chat(tokenizer, question, history=history)</span><br><span class="line">    <span class="comment"># 修剪历史并保存</span></span><br><span class="line">    new_history = trim_history(new_history)</span><br><span class="line">    save_history(session_id, new_history)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;intent&quot;</span>: intent,</span><br><span class="line">        <span class="string">&quot;response&quot;</span>: response,</span><br><span class="line">        <span class="string">&quot;session_id&quot;</span>: session_id</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="阶段4：API封装与Docker部署（耗时2天）"><a href="#阶段4：API封装与Docker部署（耗时2天）" class="headerlink" title="阶段4：API封装与Docker部署（耗时2天）"></a>阶段4：API封装与Docker部署（耗时2天）</h3><p>为了方便使用，将模型封装为HTTP接口，并通过Docker实现一键部署。</p>
<ol>
<li><strong>Flask API开发</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/chat&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat</span>():</span><br><span class="line">    data = request.get_json()</span><br><span class="line">    session_id = data.get(<span class="string">&quot;session_id&quot;</span>) <span class="keyword">or</span> generate_session_id()</span><br><span class="line">    question = data.get(<span class="string">&quot;question&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> question:</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;code&quot;</span>: <span class="number">400</span>, <span class="string">&quot;msg&quot;</span>: <span class="string">&quot;问题不能为空&quot;</span>&#125;)</span><br><span class="line">    result = roin_ai_chat(session_id, question)</span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;code&quot;</span>: <span class="number">200</span>, <span class="string">&quot;data&quot;</span>: result&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>Dockerfile编写</strong>：<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.9</span>-slim</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> requirements.txt .</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制代码和模型文件（注意：模型文件需提前放在./model目录）</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . /app</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./model /app/model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;app.py&quot;</span>]</span></span><br></pre></td></tr></table></figure></li>
<li><strong>构建并运行容器</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t roinai:v1 .</span><br><span class="line">docker run -d -p 5000:5000 --name roinai -v $(<span class="built_in">pwd</span>)/redis:/app/redis roinai:v1</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="阶段5：测试与优化（耗时2天）"><a href="#阶段5：测试与优化（耗时2天）" class="headerlink" title="阶段5：测试与优化（耗时2天）"></a>阶段5：测试与优化（耗时2天）</h3><p>完成核心开发后，进行多维度测试并优化问题：</p>
<ol>
<li><strong>功能测试</strong>：验证多轮对话、意图识别、API接口的正确性；</li>
<li><strong>性能优化</strong>：针对模型推理慢的问题，启用模型缓存、优化Redis读写逻辑；</li>
<li><strong>异常处理</strong>：添加用户输入为空、session_id无效、模型推理失败等异常的捕获和提示。</li>
</ol>
<h2 id="四、项目开发中的核心难点与解决方案"><a href="#四、项目开发中的核心难点与解决方案" class="headerlink" title="四、项目开发中的核心难点与解决方案"></a>四、项目开发中的核心难点与解决方案</h2><h3 id="难点1：上下文管理的内存与性能平衡"><a href="#难点1：上下文管理的内存与性能平衡" class="headerlink" title="难点1：上下文管理的内存与性能平衡"></a>难点1：上下文管理的内存与性能平衡</h3><p><strong>问题</strong>：随着对话轮次增加，历史上下文长度变长，模型推理耗时从初始的0.5秒逐渐增加到3秒以上，且Redis存储的历史数据量持续增长。<br><strong>解决方案</strong>：</p>
<ul>
<li>限制单会话最大对话轮次（10轮），超过后自动截断最早的对话；</li>
<li>对历史对话进行“摘要压缩”：每5轮对话，调用模型生成一段精简的上下文摘要，替换原始历史，减少上下文长度；</li>
<li>Redis设置会话过期时间（24小时），自动清理闲置会话数据。</li>
</ul>
<h3 id="难点2：意图识别的准确率不足"><a href="#难点2：意图识别的准确率不足" class="headerlink" title="难点2：意图识别的准确率不足"></a>难点2：意图识别的准确率不足</h3><p><strong>问题</strong>：初期标注的样本量少（200条），意图识别准确率仅75%，经常将“指令执行”误判为“知识问答”。<br><strong>解决方案</strong>：</p>
<ul>
<li>扩充标注样本至500条，覆盖更多边缘场景（如带语气词的指令、简短的知识问答）；</li>
<li>优化FastText训练参数：调整学习率（lr&#x3D;0.1→0.05）、增加n-gram维度（1→2）；</li>
<li>增加意图修正逻辑：若模型回复不符合意图类型，二次调用模型调整回复内容。</li>
</ul>
<h3 id="难点3：本地部署的环境兼容性问题"><a href="#难点3：本地部署的环境兼容性问题" class="headerlink" title="难点3：本地部署的环境兼容性问题"></a>难点3：本地部署的环境兼容性问题</h3><p><strong>问题</strong>：在不同电脑上部署时，出现CUDA版本不匹配、Redis启动失败、依赖包版本冲突等问题。<br><strong>解决方案</strong>：</p>
<ul>
<li>编写详细的环境配置文档，区分GPU&#x2F;CPU版本的部署步骤；</li>
<li>使用Docker容器化部署，将所有依赖和环境封装在镜像中，实现“一次构建，到处运行”；</li>
<li>增加启动自检脚本，运行前检查Redis、模型文件、依赖包是否正常，异常时给出明确提示。</li>
</ul>
<h2 id="五、项目总结与后续规划"><a href="#五、项目总结与后续规划" class="headerlink" title="五、项目总结与后续规划"></a>五、项目总结与后续规划</h2><p>RoinAI对话模型从构思到落地，耗时约10天，核心实现了多轮对话、意图识别、轻量化部署三大核心功能。整个开发过程中，我最大的收获是理解了对话系统的核心逻辑——不仅是模型的调用，更重要的是上下文管理、意图识别等配套模块的协同，以及工程化落地的细节（如性能、兼容性、易用性）。</p>
<p>后续我计划从三个方向优化RoinAI：</p>
<ol>
<li><strong>效果优化</strong>：接入更大的开源模型（如ChatGLM2-6B），提升回答的准确性和丰富性；</li>
<li><strong>功能扩展</strong>：增加自定义指令、知识库接入（如本地文档问答）功能；</li>
<li><strong>交互优化</strong>：开发简单的前端页面，让非技术用户也能便捷使用。</li>
</ol>
<p>对于想入门对话模型开发的开发者，我的建议是：不要一开始就追求复杂的功能，先从基础的模型调用、单轮对话做起，再逐步叠加上下文、意图识别等模块，每一步都做好测试，避免后期出现难以定位的问题。AI应用开发的核心是“小步快跑，快速迭代”，希望我的这次实践能给你带来一些启发。</p>
<hr>
<p>以上是完整的博客md内容，你可以直接复制使用，也可以根据实际开发细节调整内容（如技术选型、耗时、难点等）。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Aidengo.github.io">曾子复</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://aidengo.github.io/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/">https://aidengo.github.io/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Aidengo.github.io" target="_blank">Aidengo‘s blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2026/01/21/Spring-AI-Alibaba%E5%AE%9E%E6%88%98%EF%BC%9A%E9%9B%B6%E4%BB%A3%E7%A0%81%E6%90%AD%E5%BB%BARAG-%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8AI%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%81/" title="Spring AI Alibaba实战：零代码搭建RAG+工具调用AI智能体！"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Spring AI Alibaba实战：零代码搭建RAG+工具调用AI智能体！</div></div><div class="info-2"><div class="info-item-1">Spring AI Alibaba 实战指南：零代码搭建企业级 RAG+工具调用智能体 保姆级教程从入门到精通，基于 Java 生态构建高可用 AI 智能体，深度整合阿里云百炼与国产大模型  前言随着大模型技术在企业场景的深入落地，单纯的基础 AI 能力已无法满足复杂业务需求。企业级应用不仅需要对接多样化大模型，还需解决多智能体协作、知识库检索（RAG）、工具调用、可观测性等核心痛点。 Spring AI Alibaba 作为阿里巴巴基于 Spring AI 生态扩展的企业级框架，深度集成阿里云智能服务（如百炼平台、Nacos、ARMS 等），提供了从低代码到零代码的智能体构建方案，完美填补了 Java 生态中企业级 AI 解决方案的空白。 本文将以 OA 智能助手 为例，从框架认知、环境搭建、代码实现、功能测试到部署优化，全方位详细讲解如何使用 Spring AI Alibaba 构建支持 RAG 知识库与工具调用的智能体，所有代码可直接复用。 一、Spring AI Alibaba 深度解析1.1 框架定义与核心定位Spring AI Alibaba 是面向企业级场景的 AI...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">曾子复</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/https://github.com/Aidengo"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BARoinAI%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%88%91%E7%9A%84%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5%E4%B8%8E%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">从零构建RoinAI对话模型：我的技术实践与心路历程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%B8%8E%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87"><span class="toc-number">1.1.</span> <span class="toc-text">一、项目背景与核心目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%A1%B9%E7%9B%AE%E6%8A%80%E6%9C%AF%E6%A0%88%E9%80%89%E5%9E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">二、项目技术栈选型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E5%85%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">三、项目开发全流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%B6%E6%AE%B51%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E8%B0%83%E7%94%A8%EF%BC%88%E8%80%97%E6%97%B61%E5%A4%A9%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">阶段1：环境搭建与模型基础调用（耗时1天）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%B6%E6%AE%B52%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E6%A8%A1%E5%9D%97%E5%BC%80%E5%8F%91%EF%BC%88%E8%80%97%E6%97%B62%E5%A4%A9%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">阶段2：上下文管理模块开发（耗时2天）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%B6%E6%AE%B53%EF%BC%9A%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9D%97%E5%BC%80%E5%8F%91%EF%BC%88%E8%80%97%E6%97%B63%E5%A4%A9%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">阶段3：意图识别模块开发（耗时3天）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%B6%E6%AE%B54%EF%BC%9AAPI%E5%B0%81%E8%A3%85%E4%B8%8EDocker%E9%83%A8%E7%BD%B2%EF%BC%88%E8%80%97%E6%97%B62%E5%A4%A9%EF%BC%89"><span class="toc-number">1.3.4.</span> <span class="toc-text">阶段4：API封装与Docker部署（耗时2天）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%B6%E6%AE%B55%EF%BC%9A%E6%B5%8B%E8%AF%95%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E8%80%97%E6%97%B62%E5%A4%A9%EF%BC%89"><span class="toc-number">1.3.5.</span> <span class="toc-text">阶段5：测试与优化（耗时2天）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E6%A0%B8%E5%BF%83%E9%9A%BE%E7%82%B9%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.4.</span> <span class="toc-text">四、项目开发中的核心难点与解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%BE%E7%82%B91%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E7%9A%84%E5%86%85%E5%AD%98%E4%B8%8E%E6%80%A7%E8%83%BD%E5%B9%B3%E8%A1%A1"><span class="toc-number">1.4.1.</span> <span class="toc-text">难点1：上下文管理的内存与性能平衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%BE%E7%82%B92%EF%BC%9A%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%E4%B8%8D%E8%B6%B3"><span class="toc-number">1.4.2.</span> <span class="toc-text">难点2：意图识别的准确率不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%BE%E7%82%B93%EF%BC%9A%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%E7%8E%AF%E5%A2%83%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.3.</span> <span class="toc-text">难点3：本地部署的环境兼容性问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%E4%B8%8E%E5%90%8E%E7%BB%AD%E8%A7%84%E5%88%92"><span class="toc-number">1.5.</span> <span class="toc-text">五、项目总结与后续规划</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/21/Spring-AI-Alibaba%E5%AE%9E%E6%88%98%EF%BC%9A%E9%9B%B6%E4%BB%A3%E7%A0%81%E6%90%AD%E5%BB%BARAG-%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8AI%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%81/" title="Spring AI Alibaba实战：零代码搭建RAG+工具调用AI智能体！">Spring AI Alibaba实战：零代码搭建RAG+工具调用AI智能体！</a><time datetime="2026-01-21T01:57:52.000Z" title="发表于 2026-01-21 09:57:52">2026-01-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/21/RoinAI%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/" title="RoinAI智能对话模型">RoinAI智能对话模型</a><time datetime="2026-01-21T01:05:54.000Z" title="发表于 2026-01-21 09:05:54">2026-01-21</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 曾子复</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>